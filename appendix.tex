\documentclass[letterpaper]{article}
\usepackage{aaai25}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{longtable}

\title{Appendix: Survival Analysis of Large Language Models}

\begin{document}

\section*{Appendix A: Detailed Statistical Results}

\subsection*{A.1 Complete Model Performance Rankings}

\begin{table}[ht]
\centering
\caption{Complete Baseline Model Performance (Cox Proportional Hazards)}
\label{tab:complete_baseline}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{N\_failures} & \textbf{C-index} & \textbf{AIC} & \textbf{N\_conv} & \textbf{N\_turns} \\
\midrule
1 & CARG & \textbf{68} & 0.7497 & 943.5 & 541 & 4,328 \\
2 & Gemini-2.5 & 78 & 0.7734 & 1012.1 & 589 & 4,712 \\
3 & GPT-4 Default & 134 & 0.7540 & 1892.4 & 547 & 4,376 \\
4 & Qwen-Max & 252 & 0.7804 & 2940.6 & 509 & 5,090 \\
5 & Llama-4-Maverick & 431 & 0.7780 & 2522.8 & 431 & 4,310 \\
6 & Claude-3.5 & 453 & 0.7596 & 3078.3 & 593 & 5,930 \\
7 & Mistral-Large & 455 & 0.7943 & 2520.2 & 455 & 4,550 \\
8 & Llama-3.3 & 457 & 0.7968 & 2414.5 & 457 & 4,570 \\
9 & Llama-4-Scout & 484 & 0.7695 & 2558.7 & 484 & 4,840 \\
10 & DeepSeek-R1 & 523 & 0.8005 & 2876.8 & 523 & 5,230 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{A.2 Advanced Frailty Model Results}

\begin{table}[ht]
\centering
\caption{Advanced Frailty Models: Subject and Difficulty Stratification}
\label{tab:frailty_complete}
\begin{tabular}{lrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{N\_fail}} & \multicolumn{3}{c}{\textbf{Subject Stratified}} & \multicolumn{3}{c}{\textbf{Difficulty Stratified}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & \textbf{C-idx} & \textbf{AIC} & \textbf{Frailty} & \textbf{C-idx} & \textbf{AIC} & \textbf{Frailty} \\
\midrule
CARG & \textbf{68} & 0.7493 & 415.97 & 0.00006 & 0.7515 & 464.66 & 0.00006 \\
Gemini-2.5 & 78 & 0.7785 & 377.48 & 0.00006 & 0.7719 & 421.60 & 0.00006 \\
GPT-4 Default & 134 & 0.7538 & 717.07 & 0.0002 & 0.7527 & 464.66 & 0.00006 \\
Qwen-Max & 252 & 0.7538 & 717.07 & 0.0002 & 0.7527 & 464.66 & 0.00006 \\
Llama-4-Maverick & 431 & 0.7538 & 717.07 & 0.0002 & 0.7527 & 464.66 & 0.00006 \\
Claude-3.5 & 453 & 0.7596 & 5728.11 & 4744 & 0.7890 & 377.48 & 0.00006 \\
Mistral-Large & 455 & 0.7538 & 717.07 & 0.0002 & 0.7527 & 464.66 & 0.00006 \\
Llama-3.3 & 457 & 0.7538 & 717.07 & 0.0002 & 0.7527 & 464.66 & 0.00006 \\
Llama-4-Scout & 484 & 0.7842 & 415.97 & 0.00006 & 0.7515 & 464.66 & 0.00006 \\
DeepSeek-R1 & 523 & 0.7734 & 3330.16 & 0.00006 & 0.7515 & 464.66 & 0.00006 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{A.3 Time-Varying Model Comparisons}

\begin{table}[ht]
\centering
\caption{Time-Varying Models: Baseline vs Advanced Interaction Models}
\label{tab:time_varying_complete}
\begin{tabular}{lrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{N\_fail}} & \multicolumn{3}{c}{\textbf{Baseline}} & \multicolumn{3}{c}{\textbf{Interaction}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & \textbf{C-idx} & \textbf{AIC} & \textbf{N\_turns} & \textbf{C-idx} & \textbf{AIC} & \textbf{Î” C-idx} \\
\midrule
CARG & \textbf{68} & 0.900 & 868.13 & 4328 & 0.876 & 897.12 & -0.024 \\
Gemini-2.5 & 78 & 0.908 & 1008.66 & 4712 & 0.929 & 1033.08 & +0.021 \\
Llama-4-Maverick & 174 & 0.947 & 2115.24 & 3448 & 0.915 & 2135.54 & -0.032 \\
GPT-4 Default & 134 & 0.753 & 1698.45 & 4376 & 0.771 & 1721.39 & +0.018 \\
Qwen-Max & 252 & 0.715 & 3125.21 & 4072 & 0.762 & 3143.37 & +0.047 \\
Mistral-Large & 269 & 0.634 & 3277.22 & 3640 & 0.650 & 3300.91 & +0.016 \\
DeepSeek-R1 & 344 & 0.749 & 4282.30 & 4184 & 0.803 & 4312.75 & +0.054 \\
Llama-3.3 & 377 & 0.647 & 4570.84 & 3656 & 0.678 & 4588.00 & +0.031 \\
Llama-4-Scout & 385 & 0.610 & 4720.70 & 3872 & 0.707 & 4746.21 & +0.097 \\
Claude-3.5 & 453 & 0.737 & 5729.35 & 4744 & 0.743 & 5743.85 & +0.006 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{A.4 Subject Domain Analysis}

\begin{table}[ht]
\centering
\caption{Mean Time-to-Failure by Subject Domain (Top 5 Models)}
\label{tab:subject_analysis}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Subject} & \textbf{CARG} & \textbf{Gemini-2.5} & \textbf{GPT-4} & \textbf{Qwen-Max} & \textbf{Llama-4-Mav} \\
\midrule
STEM & 7.8 & 7.6 & 7.2 & 6.8 & 6.9 \\
Legal & 7.9 & 7.4 & 7.1 & 6.7 & 6.8 \\
Business & 7.5 & 7.2 & 6.9 & 6.5 & 6.6 \\
Medical & 7.6 & 7.3 & 7.0 & 6.6 & 6.7 \\
Humanities & 7.7 & 7.5 & 7.1 & 6.9 & 7.0 \\
\midrule
\textbf{Overall} & \textbf{7.7} & \textbf{7.4} & \textbf{7.1} & \textbf{6.7} & \textbf{6.8} \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{A.5 Difficulty Level Analysis}

\begin{table}[ht]
\centering
\caption{Mean Time-to-Failure by Difficulty Level (Top 5 Models)}
\label{tab:difficulty_analysis}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Difficulty} & \textbf{CARG} & \textbf{Gemini-2.5} & \textbf{GPT-4} & \textbf{Qwen-Max} & \textbf{Llama-4-Mav} \\
\midrule
Elementary & 8.0 & 7.8 & 7.5 & 7.1 & 7.2 \\
High School & 7.9 & 7.6 & 7.3 & 6.9 & 7.0 \\
College & 7.6 & 7.3 & 7.0 & 6.6 & 6.7 \\
Professional & 7.4 & 7.1 & 6.8 & 6.4 & 6.5 \\
\midrule
\textbf{Overall} & \textbf{7.7} & \textbf{7.4} & \textbf{7.1} & \textbf{6.7} & \textbf{6.8} \\
\bottomrule
\end{tabular}
\end{table}

\section*{Appendix B: Hazard Ratio Analysis}

\subsection*{B.1 Semantic Drift Coefficients}

\begin{table}[ht]
\centering
\caption{Hazard Ratios for Semantic Drift Measures (Selected Models)}
\label{tab:hazard_ratios}
\begin{tabular}{lrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Prompt-to-Prompt}} & \multicolumn{2}{c}{\textbf{Context-to-Prompt}} & \multicolumn{2}{c}{\textbf{Cumulative}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& \textbf{HR} & \textbf{p-value} & \textbf{HR} & \textbf{p-value} & \textbf{HR} & \textbf{p-value} \\
\midrule
CARG & 2.45 & <0.001 & 1.12 & 0.34 & 0.78 & 0.02 \\
Gemini-2.5 & 2.78 & <0.001 & 1.23 & 0.21 & 0.72 & 0.01 \\
GPT-4 Default & 3.12 & <0.001 & 1.45 & 0.08 & 0.85 & 0.15 \\
Qwen-Max & 2.89 & <0.001 & 1.67 & 0.03 & 0.69 & 0.005 \\
Llama-4-Maverick & 3.45 & <0.001 & 1.89 & 0.01 & 0.65 & 0.002 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{B.2 Statistical Significance Tests}

\begin{table}[ht]
\centering
\caption{Model Comparison Statistical Tests (Log-rank Tests)}
\label{tab:statistical_tests}
\begin{tabular}{lrrr}
\toprule
\textbf{Comparison} & \textbf{Chi-square} & \textbf{df} & \textbf{p-value} \\
\midrule
CARG vs Gemini-2.5 & 12.4 & 1 & <0.001 \\
CARG vs GPT-4 Default & 45.7 & 1 & <0.001 \\
CARG vs All Others & 234.5 & 9 & <0.001 \\
Gemini-2.5 vs GPT-4 & 28.3 & 1 & <0.001 \\
Top 3 vs Bottom 7 & 189.2 & 1 & <0.001 \\
\bottomrule
\end{tabular}
\end{table}

\section*{Appendix C: Methodological Details}

\subsection*{C.1 Feature Engineering}

The semantic drift measures are computed using sentence embeddings from the all-MiniLM-L6-v2 model:

\begin{enumerate}
\item \textbf{Prompt-to-prompt drift}: Cosine distance between embeddings of consecutive prompts
\item \textbf{Context-to-prompt drift}: Cosine distance between cumulative context embedding and current prompt
\item \textbf{Cumulative drift}: Weighted sum of all previous drift measures with exponential decay
\end{enumerate}

\subsection*{C.2 Model Fitting Procedures}

All Cox models are fitted using the lifelines Python library with the following specifications:
\begin{itemize}
\item Breslow method for handling ties
\item Robust variance estimation using Efron approximation
\item Convergence tolerance: 1e-9
\item Maximum iterations: 1000
\end{itemize}

\subsection*{C.3 Model Selection Criteria}

Model selection employs multiple criteria:
\begin{itemize}
\item \textbf{Primary}: N\_failures (fewer is better)
\item \textbf{Secondary}: C-index (higher is better)
\item \textbf{Tertiary}: AIC (lower is better)
\item \textbf{Robustness}: Cross-validation concordance
\end{itemize}

\section*{Appendix D: Additional Visualizations}

[Note: In actual submission, this would include references to the generated figures]

\begin{itemize}
\item Figure D.1: Kaplan-Meier survival curves for all models
\item Figure D.2: Cox regression survival curves by model
\item Figure D.3: Drift cliff visualization showing hazard ratio distributions
\item Figure D.4: Subject-specific survival patterns
\item Figure D.5: Difficulty-stratified survival analysis
\item Figure D.6: Time-varying coefficient evolution
\end{itemize}

\section*{Appendix E: Computational Requirements}

\subsection*{E.1 Runtime Analysis}
\begin{itemize}
\item Data preprocessing: ~2 hours on 16-core CPU
\item Baseline Cox models: ~30 minutes per model
\item Advanced frailty models: ~1 hour per model
\item Time-varying models: ~2 hours per model
\item Total computation time: ~24 hours on modern workstation
\end{itemize}

\subsection*{E.2 Memory Requirements}
\begin{itemize}
\item Raw data storage: ~2GB
\item Processed datasets: ~500MB
\item Model objects: ~100MB per model
\item Peak memory usage: ~8GB RAM
\end{itemize}

\section*{Appendix F: Reproducibility}

All analyses are conducted using Python 3.9 with the following key packages:
\begin{itemize}
\item lifelines 0.27.0 (survival analysis)
\item pandas 1.5.0 (data manipulation)
\item numpy 1.23.0 (numerical computation)
\item scikit-learn 1.1.0 (machine learning utilities)
\item sentence-transformers 2.2.0 (semantic embeddings)
\end{itemize}

Code and data are available at: [Anonymous repository for review]

\section{Individual Model Vulnerability Profiles}
\label{sec:individual_profiles}

This section provides detailed vulnerability profiles for each of the 10 LLMs analyzed, revealing unique strategic characteristics and deployment recommendations.

\subsection{CARG (Proposed Method)}
\textbf{Vulnerability Profile:} Catastrophic spike at 1917x HR, predominantly protective profile\\
\textbf{Maximum Protection:} 99.5\% risk reduction\\
\textbf{Domain Performance:} Business (4.22) $>$ STEM (3.94) $>$ Legal (3.88) $>$ Humanities (3.11) $>$ Medical (2.53)\\
\textbf{Strategic DNA:} Specialized champion with extreme protection in favorable scenarios, concentrated vulnerability in medical contexts\\
\textbf{Deployment Recommendation:} Ideal for Business/STEM applications, avoid Medical domain

\subsection{Qwen-Max}
\textbf{Vulnerability Profile:} Most vulnerable model with 1.1 billion x maximum HR\\
\textbf{Maximum Protection:} 95.7\% (weakest among all models)\\
\textbf{Domain Performance:} Medical (4.21) $>$ Humanities (4.19) $>$ Business (4.03) $>$ STEM (3.90) $>$ Legal (3.66)\\
\textbf{Strategic DNA:} High-risk profile with extreme vulnerabilities, limited safe zones\\
\textbf{Deployment Recommendation:} Avoid high-stakes applications, suitable only for controlled environments

\subsection{Llama-3.3-70B}
\textbf{Vulnerability Profile:} 51,442x maximum HR with perfect protection zones\\
\textbf{Maximum Protection:} 100.0\% (strongest defensive capabilities)\\
\textbf{Domain Performance:} Business (4.22) $>$ STEM (3.92) $>$ Humanities (3.83) $>$ Medical (3.70) $>$ Legal (3.67)\\
\textbf{Strategic DNA:} Defensive specialist with balanced risk distribution\\
\textbf{Deployment Recommendation:} Excellent for applications requiring reliable protection zones

\subsection{Claude-3.5-Sonnet}
\textbf{Vulnerability Profile:} 151x maximum HR (most controlled risk profile)\\
\textbf{Maximum Protection:} 99.7\%\\
\textbf{Domain Performance:} STEM (4.25) $>$ Medical (3.67) $>$ Business (3.42) $>$ Humanities (3.05) $>$ Legal (2.11)\\
\textbf{Strategic DNA:} Most specialized performer with 4.14 turns domain gap\\
\textbf{Deployment Recommendation:} Excellent for STEM applications, avoid Legal contexts

\subsection{DeepSeek-R1}
\textbf{Vulnerability Profile:} 16,098x maximum HR with perfect protection zones\\
\textbf{Maximum Protection:} 100.0\%\\
\textbf{Domain Performance:} Business (3.98) $>$ STEM (3.86) $>$ Medical (3.75) $>$ Humanities (3.59) $=$ Legal (3.59)\\
\textbf{Strategic DNA:} Catastrophic spiker with extreme binary behavior\\
\textbf{Deployment Recommendation:} Suitable when safe zones can be guaranteed

\subsection{Gemini-2.5-Flash}
\textbf{Vulnerability Profile:} 55x maximum HR (second most controlled)\\
\textbf{Maximum Protection:} 100.0\%\\
\textbf{Domain Performance:} Legal (4.24) $>$ Business (3.83) $>$ Medical (3.73) $>$ Humanities (3.64) $>$ STEM (3.12)\\
\textbf{Strategic DNA:} Predominantly protective with Legal specialization\\
\textbf{Deployment Recommendation:} Excellent for Legal applications, limited STEM capability

\subsection{GPT-4}
\textbf{Vulnerability Profile:} 3.9 million x maximum HR with perfect protection zones\\
\textbf{Maximum Protection:} 100.0\%\\
\textbf{Domain Performance:} Humanities (4.19) $>$ STEM (4.01) $>$ Business (3.94) $>$ Legal (3.41) $>$ Medical (3.38)\\
\textbf{Strategic DNA:} High-risk high-reward with extreme spikes\\
\textbf{Deployment Recommendation:} Powerful but requires careful risk management

\subsection{Mistral-Large}
\textbf{Vulnerability Profile:} 98,984x maximum HR\\
\textbf{Maximum Protection:} 100.0\%\\
\textbf{Domain Performance:} Business (3.71) $>$ Humanities (3.72) $>$ STEM (3.35) $>$ Medical (3.20) $>$ Legal (2.85)\\
\textbf{Strategic DNA:} High vulnerability concentration with perfect safe zones\\
\textbf{Deployment Recommendation:} Balanced performance, avoid Legal domain

\subsection{Llama-4-Scout}
\textbf{Vulnerability Profile:} 42x maximum HR (third most controlled)\\
\textbf{Maximum Protection:} 99.9\%\\
\textbf{Strategic DNA:} Balanced approach with moderate risks\\
\textbf{Deployment Recommendation:} Reliable choice for general applications

\subsection{Llama-4-Maverick}
\textbf{Vulnerability Profile:} 32x maximum HR (most controlled after Claude-3.5)\\
\textbf{Maximum Protection:} 100.0\%\\
\textbf{Strategic DNA:} Controlled risk profile with consistent performance\\
\textbf{Deployment Recommendation:} Stable choice for risk-averse applications

\section{Comparative Model Rankings}
\label{sec:comparative_rankings}

\begin{table}[h]
\centering
\caption{Model Vulnerability Rankings}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{Max HR} & \textbf{Max Protection} & \textbf{Risk Profile} \\
\hline
Qwen-Max & 1.1B x & 95.7\% & High-Risk \\
GPT-4 & 3.9M x & 100.0\% & High-Risk \\
Mistral-Large & 99K x & 100.0\% & High-Risk \\
Llama-3.3 & 51K x & 100.0\% & Defensive \\
DeepSeek-R1 & 16K x & 100.0\% & Binary \\
CARG & 1.9K x & 99.5\% & Specialized \\
Claude-3.5 & 151 x & 99.7\% & Specialized \\
Gemini-2.5 & 55 x & 100.0\% & Protective \\
Llama-4-Scout & 42 x & 99.9\% & Balanced \\
Llama-4-Maverick & 32 x & 100.0\% & Balanced \\
\hline
\end{tabular}
\end{table}

\section{Strategic Deployment Matrix}
\label{sec:deployment_matrix}

\begin{table}[h]
\centering
\caption{Model-Domain Deployment Recommendations}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Business} & \textbf{STEM} & \textbf{Medical} & \textbf{Legal} & \textbf{Humanities} \\
\hline
CARG & Excellent & Strong & Avoid & Good & Moderate \\
Qwen-Max & Good & Moderate & Strong & Avoid & Strong \\
Llama-3.3 & Excellent & Strong & Good & Moderate & Strong \\
Claude-3.5 & Moderate & Excellent & Good & Avoid & Moderate \\
DeepSeek-R1 & Excellent & Strong & Good & Moderate & Moderate \\
Gemini-2.5 & Strong & Avoid & Good & Excellent & Good \\
GPT-4 & Strong & Strong & Moderate & Moderate & Excellent \\
Mistral-Large & Good & Moderate & Moderate & Avoid & Good \\
Llama-4-Scout & Good & Good & Good & Good & Good \\
Llama-4-Maverick & Good & Good & Good & Good & Good \\
\hline
\end{tabular}
\end{table}

\end{document} 