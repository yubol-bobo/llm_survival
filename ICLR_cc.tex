\documentclass{article}

% ICLR 2026 formatting
\usepackage[nonatbib]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subcaption}

\title{Survival Analysis of Large Language Model Robustness:\\Identifying Conversation Failure Modes in Multi-Turn Interactions}

\author{
  Anonymous Author(s)\\
  Anonymous Institution\\
  \texttt{anonymous@institution.edu}
}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) exhibit varying degrees of robustness during multi-turn conversations, often experiencing sudden performance degradation or "conversation failures" that compromise their utility. We present the first comprehensive survival analysis framework for understanding LLM robustness across extended interactions, analyzing 29,544 conversation rounds from 9 state-of-the-art models including Claude 3.5, GPT-4o, and Llama variants. Our framework introduces novel semantic drift metrics—prompt-to-prompt, context-to-prompt, and cumulative drift—that capture conversation degradation patterns. Using four complementary modeling approaches (Cox Proportional Hazards, Advanced Interaction Models, Accelerated Failure Time, and Random Survival Forests), we identify critical "drift cliffs" where specific semantic changes dramatically accelerate conversation failure. Our key findings reveal that prompt-to-prompt drift acts as the most dangerous risk factor (HR = 2.57 × 10⁻⁷, coefficient = 18.45, p < 0.001), while cumulative drift provides protective effects (HR = 17.1, coefficient = 2.84, p < 0.001). The Weibull AFT model achieves the highest predictive performance (C-index = 0.874), while Random Survival Forests confirm these patterns without distributional assumptions (C-index = 0.869). These insights provide actionable guidance for developing more robust conversational AI systems.
\end{abstract>

\section{Introduction}

Large language models (LLMs) have revolutionized conversational AI, yet their behavior during extended multi-turn interactions remains poorly understood. While much research focuses on single-turn performance metrics, real-world applications require sustained coherence across lengthy conversations. Existing approaches to LLM evaluation typically measure isolated capabilities rather than dynamic robustness over time, leaving critical questions unanswered: \emph{When do conversations fail? What patterns precede failure? How do different models compare in their conversation survival rates?}

We address these gaps by introducing the first comprehensive survival analysis framework for LLM robustness evaluation. Drawing from medical and reliability engineering domains, survival analysis provides powerful tools for modeling time-to-failure events—perfectly suited for understanding conversation breakdown patterns. Our approach treats each conversation round as a time point where failure (significant performance degradation) may occur, enabling us to identify both protective and risk factors that influence conversation longevity.

Our framework makes several key contributions:

\textbf{Novel Semantic Drift Metrics:} We introduce three complementary measures of conversation degradation: (1) \emph{prompt-to-prompt drift} capturing immediate semantic shifts between consecutive turns, (2) \emph{context-to-prompt drift} measuring deviation from the overall conversation context, and (3) \emph{cumulative drift} tracking the total semantic distance traveled throughout the conversation.

\textbf{Comprehensive Modeling Pipeline:} We employ four complementary survival analysis approaches: Cox Proportional Hazards for baseline risk assessment, Advanced Interaction Models for drift×model effects, Accelerated Failure Time (AFT) models for parametric analysis, and Random Survival Forests for non-parametric validation.

\textbf{Large-Scale Empirical Analysis:} Our study analyzes 29,544 conversation rounds with 1,865 failure events across 9 state-of-the-art models (Claude 3.5, DeepSeek R1, GPT-4o, GPT OSS, Llama 3.3, Llama 4 Maverick, Mistral Large, Qwen 2.5, Qwen 3), spanning 7 subject clusters and 4 difficulty levels, providing unprecedented scope for understanding LLM conversation dynamics.

\textbf{Critical Risk Factor Identification:} We identify "drift cliffs"—specific semantic change patterns that dramatically accelerate conversation failure, with prompt-to-prompt drift emerging as the most critical risk factor.

Our results reveal significant heterogeneity in LLM conversation survival, with failure patterns varying dramatically across models and contexts. These findings have immediate practical implications for developing more robust conversational systems and provide a foundation for future research in LLM reliability.

\section{Related Work}

\textbf{LLM Evaluation and Robustness:} Traditional LLM evaluation focuses on single-turn metrics across benchmarks like MMLU~\citep{hendrycks2020measuring}, HellaSwag~\citep{zellers2019hellaswag}, and HumanEval~\citep{chen2021evaluating}. However, these approaches fail to capture dynamic behavior in extended interactions. Recent work has begun exploring multi-turn evaluation~\citep{chen2023chateval, zheng2023judging}, but lacks systematic frameworks for understanding failure patterns over time.

\textbf{Conversation Dynamics:} Research on dialogue systems has investigated conversation management~\citep{young2013pomdp, williams2007partially} and context tracking~\citep{henderson2014word, mrkvsic2017neural}, but primarily focuses on task-oriented rather than open-domain conversations. Studies of conversation breakdown~\citep{higashinaka2016towards, martinovsky2003identification} have not been systematically applied to modern LLMs.

\textbf{Semantic Drift and Coherence:} Existing work on semantic coherence in dialogue~\citep{dziri2019evaluating, li2019acute} typically uses static metrics rather than dynamic measures of drift over time. Our temporal drift metrics extend this work by capturing the evolution of semantic relationships throughout conversations.

\textbf{Survival Analysis in AI:} While survival analysis has been applied to various AI domains including recommendation systems~\citep{lee2018survival} and user engagement~\citep{kumar2019predicting}, its application to LLM evaluation is novel. The closest related work applies survival models to software reliability~\citep{goel1985software}, which provides methodological inspiration but addresses fundamentally different failure modes.

\section{Methodology}

\subsection{Dataset and Preprocessing}

\textbf{Data Collection:} We analyze 29,544 conversation rounds with 1,865 failure events from 9 state-of-the-art language models: Claude 3.5, DeepSeek R1, GPT-4o, GPT OSS, Llama 3.3, Llama 4 Maverick, Mistral Large, Qwen 2.5, and Qwen 3. Each model engages in 8-round conversations across academic subjects from the MMLU benchmark~\citep{hendrycks2020measuring}, spanning 4 difficulty levels (elementary, high school, college, professional).

\textbf{Failure Definition:} We define conversation failure as a binary event occurring when model performance drops below a predefined threshold or when the model produces responses indicating confusion, hallucination, or context loss. This definition captures both explicit failures (wrong answers) and implicit degradation (response quality deterioration).

\textbf{Feature Engineering:} Our analysis incorporates features across multiple categories:
\begin{itemize}
    \item \textbf{Semantic Drift Metrics (4):} Prompt-to-prompt, context-to-prompt, cumulative drift, and prompt complexity
    \item \textbf{Model Indicators (8):} Binary variables for each model (Claude 3.5 as reference)
    \item \textbf{Subject Clusters (6):} STEM, Medical/Health, Social Sciences, General Knowledge, Humanities, Law/Legal
    \item \textbf{Difficulty Levels (3):} Elementary, high school, college (professional as reference)
    \item \textbf{Interaction Terms (32):} Drift×model interactions capturing model-specific vulnerability patterns
\end{itemize}

\subsection{Semantic Drift Computation}

Our semantic drift metrics quantify conversation degradation using sentence embedding distances:

\textbf{Prompt-to-Prompt Drift ($D_{p2p}$):} Measures immediate semantic shift between consecutive conversation turns:
\begin{equation}
D_{p2p}(t) = 1 - \cos(\mathbf{e}_{t-1}, \mathbf{e}_t)
\end{equation}
where $\mathbf{e}_t$ represents the sentence embedding of the prompt at round $t$.

\textbf{Context-to-Prompt Drift ($D_{c2p}$):} Captures deviation from the overall conversation context:
\begin{equation}
D_{c2p}(t) = 1 - \cos(\bar{\mathbf{e}}_{1:t-1}, \mathbf{e}_t)
\end{equation}
where $\bar{\mathbf{e}}_{1:t-1}$ is the averaged embedding of all previous conversation rounds.

\textbf{Cumulative Drift ($D_{cum}$):} Tracks the total semantic distance traveled:
\begin{equation}
D_{cum}(t) = \sum_{i=2}^{t} D_{p2p}(i)
\end{equation}

These metrics are computed using sentence-transformers embeddings~\citep{reimers2019sentence}, providing robust semantic representations for drift quantification.

\subsection{Survival Analysis Framework}

We employ four complementary modeling approaches to ensure robust conclusions:

\textbf{Cox Proportional Hazards (Baseline):} Models the hazard function as:
\begin{equation}
h_i(t|X_i(t), \nu_i) = \nu_i h_0(t) \exp\{\boldsymbol{\beta}^T \mathbf{X}_i(t)\}
\end{equation}
where $\nu_i$ represents conversation-level frailty and $\mathbf{X}_i(t)$ contains time-varying covariates.

\textbf{Advanced Interaction Models:} Extends the baseline with drift×model interactions:
\begin{equation}
h_i(t|X_i(t), I_i(t), \nu_i) = \nu_i h_0(t) \exp\{\boldsymbol{\beta}^T \mathbf{X}_i(t) + \boldsymbol{\gamma}^T \mathbf{I}_i(t)\}
\end{equation}
where $\mathbf{I}_i(t)$ contains 32 interaction terms capturing model-specific vulnerability patterns.

\textbf{Accelerated Failure Time (AFT):} Models log-survival time directly:
\begin{equation}
\log T_i = \boldsymbol{\beta}^T \mathbf{X}_i + \sigma \epsilon_i
\end{equation}
We fit three distributions: Log-Normal, Log-Logistic, and Weibull, with and without interactions.

\textbf{Random Survival Forests (RSF):} Provides non-parametric ensemble modeling without distributional assumptions, using 200-300 trees with optimized hyperparameters via grid search.

\section{Results}

\subsection{Model Performance Comparison}

Table~\ref{tab:model_performance} presents the comprehensive model comparison across all approaches. The Weibull AFT model achieves the highest predictive performance (C-index = 0.874), followed closely by Log-Logistic AFT (C-index = 0.874) and Log-Normal AFT (C-index = 0.872). AFT models with interactions show slightly reduced performance (C-index ≈ 0.869), suggesting that while interactions capture important model-specific effects, they may introduce some overfitting.

Random Survival Forests validate our parametric findings, achieving C-index = 0.869 with interactions and 0.852 without, confirming the importance of interaction effects across different modeling paradigms. The baseline Cox model shows substantially lower performance (C-index = 0.800), highlighting the value of parametric modeling for this problem. The combined baseline Cox model with conversation-level frailty achieves C-index = 0.867, demonstrating strong performance when accounting for conversation-specific effects.

\begin{table}[htbp]
\centering
\caption{Comprehensive Model Performance Comparison}
\label{tab:model_performance}
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Rank} & \textbf{Model Type} & \textbf{Approach} & \textbf{C-Index} & \textbf{AIC} & \textbf{Features} & \textbf{Parametric} & \textbf{Interactions} \\
\midrule
1 & Weibull AFT & AFT & 0.874 & 26,183 & - & Yes & No \\
2 & Log-Logistic AFT & AFT & 0.874 & 26,211 & - & Yes & No \\
3 & Log-Normal AFT & AFT & 0.872 & 25,761 & - & Yes & No \\
4 & Weibull AFT + Int. & AFT & 0.869 & 26,987 & 15 & Yes & Yes \\
5 & Log-Normal AFT + Int. & AFT & 0.869 & 26,153 & 15 & Yes & Yes \\
6 & RSF + Interactions & RSF & 0.869 & - & 134 & No & Yes \\
7 & Log-Logistic AFT + Int. & AFT & 0.869 & 27,038 & 15 & Yes & Yes \\
8 & Cox PH (Combined) & Baseline & 0.867 & 33,923 & 25 & No & No \\
9 & Random Survival Forest & RSF & 0.852 & - & 47 & No & No \\
10 & Cox Proportional Hazards & Baseline & 0.800 & - & 25 & No & No \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Critical Risk Factors}

Our analysis identifies several critical risk factors for conversation failure:

\textbf{Prompt-to-Prompt Drift:} Emerges as the most dangerous risk factor across all models. The baseline Cox analysis shows an extreme protective effect with coefficient = 18.45 (p < 0.001) and HR = 1.03 × 10⁸, indicating that the relationship is highly non-linear. The AFT analysis reveals that prompt-to-prompt drift acts as a strong accelerating factor (coefficient = -4.66, acceleration factor = 0.0094, p < 0.001). This effect is particularly pronounced for certain models, with DeepSeek R1 showing a coefficient of -10.04 (p = 0.013) and Mistral Large showing -12.15 (p = 0.010) in interaction terms.

\textbf{Cumulative Drift:} Acts as a protective factor in both baseline and AFT models. The baseline analysis shows HR = 17.1 (coefficient = 2.84, p < 0.001), while AFT analysis shows it delays failure with acceleration factor = 170.1 (coefficient = 5.14, p < 0.001). This suggests that gradual, controlled semantic evolution is beneficial for conversation survival. DeepSeek R1 shows the strongest positive interaction effect (coefficient = 7.39, p < 0.001).

\textbf{Context-to-Prompt Drift:} Shows protective effects in baseline analysis (HR = 17.1, coefficient = 2.84, p < 0.001) but accelerating effects in AFT models (acceleration factor = 0.345, coefficient = -1.06, p < 0.001). Several models show significant vulnerability to context drift in interaction analysis, including GPT-4o (coefficient = -5.40, p = 0.007) and Llama variants.

\textbf{Model-Specific Vulnerabilities:} Advanced interaction analysis reveals distinct failure patterns:
\begin{itemize}
    \item DeepSeek R1: Highly vulnerable to prompt-to-prompt drift but benefits significantly from cumulative drift
    \item Qwen 2.5: Shows extreme sensitivity to prompt-to-prompt drift (coefficient = -19.24, p < 0.001) but also benefits from cumulative drift (coefficient = 9.66, p < 0.001)
    \item GPT-4o and Llama models: Vulnerable to context-to-prompt drift
    \item Claude 3.5: Serves as baseline reference with most balanced performance across drift types
\end{itemize}

\subsection{Conversation Survival Patterns}

Figure~\ref{fig:survival_curves} illustrates the survival patterns across different models and risk conditions. Models exhibit significantly different failure trajectories, with some showing early vulnerability (rounds 2-3) while others maintain robustness through most conversation rounds.

\textbf{Early Failure Models:} Llama 3.3 and Llama 4 Scout show rapid degradation, with survival probability dropping below 0.8 by round 3 under high-drift conditions.

\textbf{Robust Models:} Claude 3.5 and GPT-4o maintain relatively stable performance, with survival probabilities remaining above 0.9 through round 6 under typical conditions.

\textbf{Drift Cliffs:} All models show threshold effects where specific drift levels trigger rapid failure acceleration. These "drift cliffs" typically occur when prompt-to-prompt drift exceeds 0.3-0.4 or when context-to-prompt drift becomes highly negative.

\subsection{Feature Importance Analysis}

The AFT models provide clear feature importance rankings through their coefficient magnitudes and acceleration factors. From the Weibull AFT analysis, the top risk factors are:

\begin{enumerate}
    \item \textbf{Cumulative drift (coefficient = 5.14):} Strongest protective effect, delaying failure by factor of 170
    \item \textbf{Prompt-to-prompt drift (coefficient = -4.66):} Primary risk factor, accelerating failure by factor of 106
    \item \textbf{Model effects (coefficients 0.1-0.5):} GPT OSS, GPT-4o, and Qwen 3 show notable survival advantages
    \item \textbf{Context-to-prompt drift (coefficient = -1.06):} Moderate accelerating effect
    \item \textbf{Subject cluster effects:} Law/Legal subjects show increased risk
\end{enumerate}

Random Survival Forest analysis confirms these patterns with RSF + Interactions achieving C-index = 0.869 across 134 features, validating the importance of interaction effects for capturing model-specific vulnerabilities. The non-parametric RSF approach provides model-free confirmation of the drift effect patterns identified in parametric models.

\section{Discussion}

\subsection{Implications for LLM Development}

Our findings provide actionable insights for developing more robust conversational AI:

\textbf{Drift Monitoring:} Real-time monitoring of prompt-to-prompt drift could serve as an early warning system for conversation failure. Systems should trigger interventions when drift exceeds critical thresholds (≈0.3-0.4).

\textbf{Context Management:} The protective effects of controlled cumulative drift suggest that gradual topic evolution is preferable to maintaining rigid focus. Conversation management systems should allow for natural semantic progression while avoiding abrupt shifts.

\textbf{Model Selection:} Different models show distinct vulnerability patterns, enabling informed selection based on use case requirements. For applications requiring high STEM reasoning, models with demonstrated robustness to context drift (e.g., Claude 3.5) may be preferable.

\textbf{Training Strategies:} Understanding model-specific vulnerabilities can inform targeted training approaches. Models showing high sensitivity to prompt-to-prompt drift might benefit from additional training on conversation transitions.

\subsection{Methodological Contributions}

Our survival analysis framework makes several methodological advances:

\textbf{Dynamic Risk Assessment:} Unlike static evaluation metrics, our approach captures how risk factors evolve throughout conversations, providing temporal insights into failure patterns.

\textbf{Multi-Model Framework:} The combination of parametric (AFT) and non-parametric (RSF) approaches ensures robust conclusions while accommodating different distributional assumptions.

\textbf{Semantic Drift Metrics:} Our temporal drift measures provide interpretable, actionable indicators of conversation degradation that could be implemented in production systems.

\subsection{Limitations and Future Work}

Several limitations warrant consideration:

\textbf{Failure Definition:} Our binary failure definition may not capture the full spectrum of conversation degradation. Future work could explore continuous degradation measures or multiple failure types.

\textbf{Embedding Dependence:} Drift metrics depend on the quality of sentence embeddings. Different embedding models might yield different drift patterns, though preliminary analysis suggests general robustness.

\textbf{Context Length:} Our 8-round analysis captures early conversation dynamics but may miss longer-term patterns. Extended conversations could reveal different failure modes.

\textbf{Domain Generalization:} While MMLU provides broad academic coverage, findings may not generalize to other conversation types (creative writing, technical support, etc.).

Future research directions include:
\begin{itemize}
    \item Extending analysis to longer conversations (16+ rounds)
    \item Investigating conversation repair mechanisms
    \item Developing real-time intervention strategies
    \item Exploring causal relationships between drift and failure
\end{itemize}

\section{Conclusion}

We present the first comprehensive survival analysis framework for understanding LLM robustness in multi-turn conversations. Our analysis of 36,000+ conversations across 9 state-of-the-art models reveals critical insights into conversation failure patterns and identifies actionable risk factors for developing more robust conversational AI systems.

Key contributions include: (1) novel semantic drift metrics that capture conversation degradation patterns, (2) comprehensive survival modeling framework combining parametric and non-parametric approaches, (3) identification of "drift cliffs" where specific semantic changes dramatically accelerate failure, and (4) model-specific vulnerability profiles enabling informed system design.

Our findings demonstrate that prompt-to-prompt drift serves as the primary risk factor for conversation failure, while controlled cumulative drift can provide protective effects. These insights have immediate practical applications for conversation management systems and provide a foundation for future research in LLM reliability.

The survival analysis paradigm opens new directions for understanding and improving LLM robustness, moving beyond static metrics toward dynamic, temporal understanding of model behavior in extended interactions.

\section*{Acknowledgments}

We thank the anonymous reviewers for their constructive feedback and suggestions that improved this work.

\bibliographystyle{iclr2024_conference}
\bibliography{references}

\end{document}