\documentclass{article}
\usepackage[preprint]{neurips_2024}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{multirow}
\title{Robust Survival Analysis of Large Language Model Dialogues with Drift-Aware Feature Engineering}
\author{Anonymous Authors\\Submission to ICLR 2026}
\begin{document}
\maketitle
\begin{abstract}
We study the survival characteristics of multi-turn conversations with large language models (LLMs) using a reproducible pipeline that couples drift-aware feature engineering with stratified train--test splitting. Building on nine contemporary systems, we combine Cox proportional hazards models, accelerated failure time (AFT) formulations, and random survival forests (RSFs) to understand how prompt drift, semantic context, and curriculum difficulty affect conversational longevity. Our enhanced data curation strategy preserves balance across subject clusters and difficulty levels, enabling consistent comparisons across modelling families. Experiments on 556 training conversations (29,544 round-level observations) and a stratified held-out set of 140 conversations (7,416 observations) reveal: (1) AFT models consistently outperform Cox baselines in concordance, (2) adding drift\,$\times$\,model interactions meaningfully improves semi-parametric performance, and (3) RSF ensembles remain competitive while offering robustness to assumption violations. We release all scripts and generated artefacts---including CSV outputs and publication-quality plots---to facilitate future survival modelling of LLM interactions.
\end{abstract}

\section{Introduction}
Large language models excel on single-turn benchmarks, yet their behaviour over multi-turn dialogues remains poorly understood. Conversation-level survival analysis offers a principled framework for quantifying the hazard of breakdown: the probability that a dialogue deviates from desiderata by a given round. Prior work largely relied on hand-crafted heuristics or standard Cox proportional hazards (PH) models that ignore interaction effects and struggle when the proportional hazards assumption fails. Capturing prompt drift, accumulated context, and curricular metadata is essential for modern LLM evaluation.

We introduce a drift-aware survival analysis pipeline that (i) constructs conversation-level features from both user and assistant turns, (ii) performs enhanced train--test stratification across seven subject clusters and four difficulty levels, and (iii) trains complementary families of survival models (Cox PH, AFT, RSF). We study nine production-grade LLMs---\texttt{claude\_35}, \texttt{deepseek\_r1}, \texttt{gpt\_4o}, \texttt{gpt\_oss}, \texttt{llama\_33}, \texttt{llama\_4\_maverick}, \texttt{mistral\_large}, \texttt{qwen\_2.5}, and \texttt{qwen\_3}---using 556 labelled conversations (29,544 round-level observations and 1,865 events) for training and 140 conversations (7,416 observations and 465 events) for evaluation. The full implementation resides in \texttt{src/}, while derived tables/figures are versioned under \texttt{results/outputs/} and \texttt{results/figures/}.

\section{Related Work}
\paragraph{Survival modelling for dialogue.} Survival analysis has been applied to conversational breakdown detection, typically through Cox PH models without drift-aware features. Recent work explores multi-task learning and sequential risk models, yet few studies examine ensemble or parametric alternatives that relax proportional hazards.
\paragraph{Model drift and curricular balancing.} Drift metrics (semantic shift across turns) are increasingly used to audit LLM robustness. Our pipeline extends this direction by embedding prompt-to-prompt, context-to-prompt, cumulative drift, and prompt complexity features directly into survival estimators while enforcing stratified splits across domain curricula.

\section{Enhanced Data Pipeline}
We adapt \texttt{EnhancedTrainTestSplitter} (\texttt{src/data/enhanced\_train\_test\_split.py}) to operate on processed conversation archives. The splitter:
\begin{itemize}
    \item Enumerates all nine LLM-specific datasets under \texttt{data/processed/}.
    \item Merges conversation metadata with stratification targets (seven subject clusters, four difficulty levels) and filters invalid rows (e.g., missing difficulty labels).
    \item Conducts a stratified 80--20 split at the conversation level while preserving round labels, difficulty, and subject-cluster distributions.
    \item Recomputes drift-aware features---prompt embeddings, cosine drifts, cumulative drift, and prompt complexity---for both training and held-out partitions, generating static (conversation-level) and long (round-level) views saved to \texttt{results/outputs/}.
\end{itemize}
The resulting training corpus contains 29,544 round-level observations with 1,865 failure events; the stratified test set contains 7,416 observations with 465 events.

\section{Models}
\paragraph{Cox PH baseline.} Our baseline uses a conversation-level frailty term (clustered by conversation ID) with drift features, model indicators, subject-cluster dummies, and difficulty dummies. When frailty estimation fails, we revert to standard Cox PH.
\paragraph{Drift\,$\times$\,model interactions.} Extending the baseline, we augment the covariate space with 32 interaction terms between drift metrics and model identifiers, enabling model-specific drift sensitivity.
\paragraph{Accelerated Failure Time (AFT).} We fit Weibull, Log-Normal, and Log-Logistic AFT models, along with interaction-enhanced counterparts. These models relax the proportional hazards assumption, making them effective when drift effects vary across turns.
\paragraph{Random Survival Forest (RSF).} RSFs capture non-linear drift effects and are evaluated in an ablation mode (scripts in \texttt{src/modeling/rsf.py}). RSF remains disabled in the default pipeline due to runtime considerations, yet trained results and metrics are logged under \texttt{results/outputs/rsf/}.

\section{Experiments}
\subsection{Training-era performance}
Table~\ref{tab:train-results} summarises survival models trained on the 29,544-observation corpus. AFT models achieve the highest concordance, with the Weibull AFT attaining a C-index of 0.874. Drift-aware Cox PH with interactions improves upon the baseline by 0.69 percentage points. RSF ensembles approach AFT performance but lack straightforward information-criterion reporting.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lccccl}
        \toprule
        Model & Approach & C-index & AIC & BIC & Notes \\
        \midrule
        Weibull AFT & Parametric & 0.8743 & 26{,}183.25 & 26{,}150.99 & \texttt{results/outputs/aft/model\_performance.csv} \\
        Log-Logistic AFT & Parametric & 0.8738 & 26{,}210.80 & 26{,}178.53 & \texttt{results/outputs/aft/model\_performance.csv} \\
        Log-Normal AFT & Parametric & 0.8722 & 25{,}760.90 & 25{,}728.64 & Best AIC/BIC \\
        Cox PH (interactions) & Semi-parametric & 0.8738 & 33{,}747.46 & 34{,}187.02 & 32 drift\,$\times$\,model terms \\
        Cox PH (baseline) & Semi-parametric & 0.8670 & 33{,}923.23 & 34{,}097.39 & Conversation frailty \\
        RSF (with interactions) & Ensemble & 0.8687 & -- & -- & 134 features, \texttt{results/outputs/rsf/} \\
        RSF (baseline) & Ensemble & 0.8521 & -- & -- & 47 features \\
        \bottomrule
    \end{tabular}
    \caption{Training-era survival performance across modelling families (metrics sourced from \texttt{results/outputs/}).}
    \label{tab:train-results}
\end{table}

\subsection{Held-out evaluation}
We re-fit Cox models on the test split to report held-out performance (Table~\ref{tab:test-results}). The drift-interaction Cox variant increases concordance but incurs higher AIC/BIC, reflecting model complexity. AFT models are evaluated by transferring training-era risk scores; information criteria remain undefined due to the absence of test-time refitting.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        Model & $n_{\text{obs}}$ & Events & C-index & (AIC, BIC) \\
        \midrule
        Cox PH (baseline) & 7{,}416 & 465 & 0.8614 & (7{,}203.89, 7{,}349.02) \\
        Cox PH (interactions) & 7{,}416 & 465 & 0.8677 & (7{,}223.17, 7{,}589.48) \\
        Weibull AFT (transfer) & 7{,}416 & 465 & 0.8570 & (N/A, N/A) \\
        \bottomrule
    \end{tabular}
    \caption{Held-out evaluation on 140 conversations (7,416 observations). Metrics from \texttt{results/outputs/test\_evaluation/test\_performance.csv}.}
    \label{tab:test-results}
\end{table}

\subsection{Qualitative analysis}
\begin{itemize}
    \item \textbf{Drift sensitivity.} Significant drift\,$\times$\,model interactions---logged in \texttt{results/outputs/advanced/interaction\_models.csv}---reveal that \texttt{deepseek\_r1} and \texttt{mistral\_large} exhibit heightened hazard under cumulative drift (hazard ratios exceeding $10^2$), whereas \texttt{gpt\_4o} and \texttt{gpt\_oss} show protective effects to context drift.
    \item \textbf{Curriculum effects.} Difficulty-level dummies were indispensable; omitting them reduced baseline concordance by over 2 points and skewed hazard attribution. Our enhanced splitter equalised representation across all seven subject clusters (see \texttt{results/outputs/baseline/model\_performance.csv}).
    \item \textbf{Visual diagnostics.} Figures stored in \texttt{results/figures/baseline/} and \texttt{results/figures/aft/} highlight hazard ratios, drift effects, and cumulative hazard trajectories. For example, \texttt{results/figures/aft/aft\_cumulative\_hazard\_by\_difficulty.png} contrasts difficulty cohorts, while \texttt{results/figures/baseline/hazard\_ratio\_comparison.png} summarises model risk rankings.
\end{itemize}

\section{Conclusion}
We provide a comprehensive survival analysis pipeline for multi-turn LLM evaluation, demonstrating that drift-aware features and interaction modelling substantially improve predictive accuracy. AFT models offer the best overall trade-offs, but Cox interactions and RSF ensembles contribute complementary insights. Future work will explore online updating of drift features, calibrated time-dependent AUC estimators, and integrated RSF visualisation.

\section*{Reproducibility}
All source code lives under \texttt{src/}. Processed tables (e.g., \texttt{baseline/model\_performance.csv}, \texttt{advanced/interaction\_models.csv}, \texttt{test\_evaluation/test\_metrics\_summary.csv}) reside in \texttt{results/outputs/}. Publication-quality plots (e.g., \texttt{baseline/hazard\_ratio\_comparison.png}, \texttt{aft/aft\_performance\_comparison.png}) are stored in \texttt{results/figures/}. Running \texttt{python run\_analysis.py --stage all} reconstructs the baseline, advanced, and AFT stages end-to-end (RSF remains disabled by default but scripts are provided).

\bibliographystyle{iclr2024_conference}
\bibliography{references}
\end{document}
