#!/usr/bin/env python3
"""
LLMç”Ÿå­˜åˆ†æç‰¹å¾æ„æˆå…¨é¢åˆ†ææŠ¥å‘Š
åˆ†æCoxå›å½’æ¨¡å‹ä¸­çš„15ä¸ªåŸºç¡€ç‰¹å¾ä»¥åŠé«˜çº§äº¤äº’é¡¹å’Œæ—¶é—´å˜åŒ–åå˜é‡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def comprehensive_feature_analysis():
    """å…¨é¢åˆ†æLLMç”Ÿå­˜åˆ†æä¸­çš„ç‰¹å¾æ„æˆ"""
    
    print("ğŸ” LLMç”Ÿå­˜åˆ†æç‰¹å¾æ„æˆå…¨é¢åˆ†æ")
    print("="*80)
    
    # 1. åŸºç¡€15ä¸ªç‰¹å¾åˆ†æ
    print("\nğŸ“Š **1. åŸºç¡€15ä¸ªç‰¹å¾åˆ†æ**")
    print("-"*50)
    
    basic_features = {
        "æ¼‚ç§»ç‰¹å¾ (4ä¸ª)": [
            "prompt_to_prompt_drift",    # æç¤ºé—´æ¼‚ç§»
            "context_to_prompt_drift",   # ä¸Šä¸‹æ–‡åˆ°æç¤ºæ¼‚ç§»  
            "cumulative_drift",          # ç´¯ç§¯æ¼‚ç§»
            "prompt_complexity"          # æç¤ºå¤æ‚åº¦
        ],
        "æ¨¡å‹è™šæ‹Ÿå˜é‡ (11ä¸ª)": [
            "model_deepseek_r1",         # DeepSeek R1
            "model_gemini_25",           # Gemini 2.5
            "model_gpt_4o",             # GPT-4o
            "model_gpt_5",              # GPT-5
            "model_gpt_oss",            # GPT OSS
            "model_llama_33",           # LLaMA 3.3
            "model_llama_4_maverick",   # LLaMA 4 Maverick
            "model_llama_4_scout",      # LLaMA 4 Scout
            "model_mistral_large",      # Mistral Large
            "model_qwen_3",             # Qwen 3.0
            "model_qwen_max"            # Qwen Max
        ]
    }
    
    for category, features in basic_features.items():
        print(f"\n**{category}:**")
        for i, feature in enumerate(features, 1):
            print(f"   {i}. {feature}")
    
    print(f"\n**æ€»è®¡:** {sum(len(features) for features in basic_features.values())} ä¸ªåŸºç¡€ç‰¹å¾")
    
    # 2. é«˜çº§äº¤äº’é¡¹åˆ†æ
    print("\nâš¡ **2. é«˜çº§äº¤äº’é¡¹åˆ†æ**")
    print("-"*50)
    
    try:
        interaction_df = pd.read_csv('results/outputs/advanced/interaction_effects.csv')
        print(f"**å‘ç° {len(interaction_df)} ä¸ªäº¤äº’é¡¹ï¼**")
        
        # æŒ‰æ˜¾è‘—æ€§åˆ†ç»„
        significant_interactions = interaction_df[interaction_df['significant'] == True]
        print(f"   - æ˜¾è‘—äº¤äº’é¡¹: {len(significant_interactions)} ä¸ª")
        print(f"   - éæ˜¾è‘—äº¤äº’é¡¹: {len(interaction_df) - len(significant_interactions)} ä¸ª")
        
        print(f"\n**æ˜¾è‘—äº¤äº’é¡¹å‰5ä¸ª:**")
        top_interactions = significant_interactions.nlargest(5, 'hazard_ratio')
        for idx, row in top_interactions.iterrows():
            print(f"   â€¢ {row['interaction_term']}")
            print(f"     HR = {row['hazard_ratio']:.2e}, p = {row['p_value']:.6f}")
        
        print(f"\n**äº¤äº’é¡¹ç±»å‹åˆ†å¸ƒ:**")
        interaction_types = {}
        for term in interaction_df['interaction_term']:
            if 'prompt_to_prompt_drift_x_model' in term:
                interaction_types['P2Pæ¼‚ç§»Ã—æ¨¡å‹'] = interaction_types.get('P2Pæ¼‚ç§»Ã—æ¨¡å‹', 0) + 1
            elif 'context_to_prompt_drift_x_model' in term:
                interaction_types['C2Pæ¼‚ç§»Ã—æ¨¡å‹'] = interaction_types.get('C2Pæ¼‚ç§»Ã—æ¨¡å‹', 0) + 1
            elif 'cumulative_drift_x_model' in term:
                interaction_types['ç´¯ç§¯æ¼‚ç§»Ã—æ¨¡å‹'] = interaction_types.get('ç´¯ç§¯æ¼‚ç§»Ã—æ¨¡å‹', 0) + 1
            elif 'prompt_complexity_x_model' in term:
                interaction_types['å¤æ‚åº¦Ã—æ¨¡å‹'] = interaction_types.get('å¤æ‚åº¦Ã—æ¨¡å‹', 0) + 1
        
        for itype, count in interaction_types.items():
            print(f"   â€¢ {itype}: {count} ä¸ª")
            
    except FileNotFoundError:
        print("   âŒ äº¤äº’é¡¹æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°")
    
    # 3. æ—¶é—´å˜åŒ–åå˜é‡åˆ†æ  
    print("\nâ° **3. æ—¶é—´å˜åŒ–åå˜é‡åˆ†æ**")
    print("-"*50)
    
    try:
        time_varying_df = pd.read_csv('results/outputs/time_varying/time_varying_models.csv')
        print(f"**å‘ç° {len(time_varying_df)} ä¸ªæ¨¡å‹çš„æ—¶é—´å˜åŒ–åˆ†æï¼**")
        
        # åˆ†ææ—¶é—´å˜åŒ–ç‰¹å¾
        tv_models = time_varying_df['model'].tolist()
        print(f"   - åŒ…å«æ¨¡å‹: {', '.join(tv_models)}")
        
        # C-indexæ€§èƒ½
        avg_cindex = time_varying_df['c_index'].mean()
        print(f"   - å¹³å‡C-index: {avg_cindex:.4f}")
        
        print(f"\n**æ—¶é—´å˜åŒ–ç‰¹å¾æ•ˆåº”:**")
        for col in ['prompt_to_prompt_drift', 'context_to_prompt_drift', 'cumulative_drift', 'prompt_complexity']:
            coef_col = f"{col}_coef"
            pval_col = f"{col}_pval"
            if coef_col in time_varying_df.columns:
                avg_coef = time_varying_df[coef_col].mean()
                avg_pval = time_varying_df[pval_col].mean()
                print(f"   â€¢ {col}: å¹³å‡ç³»æ•° = {avg_coef:.6f}, å¹³å‡på€¼ = {avg_pval:.4f}")
        
        # æ£€æŸ¥äº¤äº’æ—¶é—´å˜åŒ–
        interaction_tv_df = pd.read_csv('results/outputs/time_varying/interaction_time_varying_results.csv')
        print(f"\n**äº¤äº’Ã—æ—¶é—´å˜åŒ–åˆ†æ:**")
        print(f"   - æ€»åå˜é‡æ•°: {len(interaction_tv_df)}")
        
        unique_models = interaction_tv_df['Model'].nunique()
        print(f"   - æ¶‰åŠæ¨¡å‹æ•°: {unique_models}")
        
        significant_tv = interaction_tv_df[interaction_tv_df['p'] < 0.05]
        print(f"   - æ˜¾è‘—æ—¶é—´å˜åŒ–æ•ˆåº”: {len(significant_tv)} ä¸ª")
        
    except FileNotFoundError:
        print("   âŒ æ—¶é—´å˜åŒ–æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°")
    
    # 4. ç‰¹å¾å¤æ‚åº¦æ€»ç»“
    print("\nğŸ¯ **4. ç‰¹å¾å¤æ‚åº¦å±‚æ¬¡æ€»ç»“**")
    print("-"*50)
    
    feature_hierarchy = {
        "ç¬¬1å±‚ - åŸºç¡€ç‰¹å¾": {
            "æ•°é‡": 15,
            "ç±»å‹": "4ä¸ªæ¼‚ç§»ç‰¹å¾ + 11ä¸ªæ¨¡å‹è™šæ‹Ÿå˜é‡",
            "è¯´æ˜": "æ ‡å‡†Coxå›å½’çš„åŸºç¡€åå˜é‡"
        },
        "ç¬¬2å±‚ - äº¤äº’æ•ˆåº”": {
            "æ•°é‡": "44ä¸ª",
            "ç±»å‹": "æ¼‚ç§»ç‰¹å¾ Ã— æ¨¡å‹äº¤äº’é¡¹",
            "è¯´æ˜": "æ•æ‰ä¸åŒæ¨¡å‹å¯¹æ¼‚ç§»çš„å·®å¼‚åŒ–å“åº”"
        },
        "ç¬¬3å±‚ - æ—¶é—´å˜åŒ–": {
            "æ•°é‡": "270+ä¸ª",
            "ç±»å‹": "åŸºç¡€ç‰¹å¾çš„æ—¶é—´ç›¸å…³å˜åŒ–",
            "è¯´æ˜": "å…è®¸åå˜é‡æ•ˆåº”éšæ—¶é—´å˜åŒ–"
        },
        "ç¬¬4å±‚ - äº¤äº’Ã—æ—¶é—´": {
            "æ•°é‡": "å¤åˆå‹",
            "ç±»å‹": "äº¤äº’é¡¹çš„æ—¶é—´å˜åŒ–æ•ˆåº”",
            "è¯´æ˜": "æœ€å¤æ‚çš„åŠ¨æ€äº¤äº’æ¨¡å¼"
        }
    }
    
    for layer, info in feature_hierarchy.items():
        print(f"\n**{layer}:**")
        print(f"   â€¢ ç‰¹å¾æ•°é‡: {info['æ•°é‡']}")
        print(f"   â€¢ ç‰¹å¾ç±»å‹: {info['ç±»å‹']}")
        print(f"   â€¢ åŠŸèƒ½è¯´æ˜: {info['è¯´æ˜']}")
    
    # 5. å»ºè®®çš„æ‰©å±•ç‰¹å¾
    print("\nğŸ’¡ **5. å»ºè®®çš„æ‰©å±•ç‰¹å¾**")
    print("-"*50)
    
    extensions = {
        "æ—¶é—´å¤šé¡¹å¼ç‰¹å¾": [
            "round^2, round^3 (éçº¿æ€§æ—¶é—´æ•ˆåº”)",
            "log(round), sqrt(round) (æ—¶é—´å˜æ¢)"
        ],
        "æ¼‚ç§»é—´äº¤äº’": [
            "prompt_to_prompt_drift Ã— cumulative_drift",
            "context_to_prompt_drift Ã— prompt_complexity"
        ],
        "åˆ†å±‚æ—¶é—´æ•ˆåº”": [
            "early_phase (round 1-2) Ã— features",
            "late_phase (round 3-4) Ã— features"
        ],
        "åŠ¨æ€é˜ˆå€¼ç‰¹å¾": [
            "drift_threshold_crossed (æ¼‚ç§»è¶…è¿‡é˜ˆå€¼çš„äºŒå…ƒå˜é‡)",
            "complexity_percentile (å¤æ‚åº¦åœ¨å¯¹è¯ä¸­çš„ç›¸å¯¹ä½ç½®)"
        ],
        "åºåˆ—ä¾èµ–": [
            "lagged_drift (å‰ä¸€è½®çš„æ¼‚ç§»å€¼)",
            "drift_acceleration (æ¼‚ç§»å˜åŒ–ç‡)"
        ]
    }
    
    for ext_type, features in extensions.items():
        print(f"\n**{ext_type}:**")
        for feature in features:
            print(f"   â€¢ {feature}")
    
    print("\n" + "="*80)
    print("ğŸ‰ **ç‰¹å¾åˆ†æå®Œæˆï¼å½“å‰æ¨¡å‹å·²ä»15ä¸ªåŸºç¡€ç‰¹å¾æ‰©å±•åˆ°300+ä¸ªå¤åˆç‰¹å¾**")
    print("="*80)

if __name__ == "__main__":
    comprehensive_feature_analysis()