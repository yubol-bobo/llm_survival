% File: main.tex
\documentclass[letterpaper]{article}
\usepackage{aaai24}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\urlstyle{rm}
\def\UrlFont{\rm}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\title{Survival Analysis of Large Language Model Robustness to Adversarial Prompting}

\author{
    Your Name\textsuperscript{\rm 1}, Collaborator Name\textsuperscript{\rm 2} \\
}
\affiliations {
    \textsuperscript{\rm 1}Your Institution, \textsuperscript{\rm 2}Collaborator Institution \\
    your.email@domain.com, collaborator@domain.com
}

\begin{document}
\maketitle

\begin{abstract}
Large language models (LLMs) are increasingly deployed in high-stakes settings, yet their robustness to adversarial prompting remains poorly understood. We propose a novel experimental and statistical framework for quantifying LLM robustness using survival analysis and count regression. Our approach models the number of adversarial turns until failure and the time-to-event (failure) as a function of model, prompt, and semantic drift. We introduce frailty models to account for latent conversation-level vulnerability. Experiments on diverse LLMs and prompt sets demonstrate the value of our approach for benchmarking and improving LLM safety.
\end{abstract}

\section{Introduction}
Large language models (LLMs) have achieved remarkable performance across a range of tasks, but their susceptibility to adversarial prompts poses significant risks. Existing robustness benchmarks often focus on single-turn attacks or aggregate error rates, failing to capture the sequential, dynamic nature of adversarial interactions. We address this gap by modeling LLM robustness as a survival process: how many adversarial turns can a model withstand before producing an incorrect answer?

\section{Related Work}
Recent work has highlighted the vulnerability of LLMs to adversarial attacks \cite{zou2023universal, wei2023jailbroken, perez2022red}. Most benchmarks focus on static or single-turn adversarial examples \cite{hendrycks2021measuring, liang2023holistic}, or aggregate error rates across tasks. Survival analysis has been used in other domains for time-to-event modeling \cite{kleinbaum2012survival}, but its application to LLM robustness is novel. Our work builds on advances in sequential adversarial evaluation \cite{liu2023llm}, semantic drift measurement \cite{reimers2019sentence}, and mixed-effects modeling \cite{bates2015fitting} to provide a comprehensive framework for LLM robustness assessment.

\section{Methods}
We model the adversarial robustness of LLMs as a survival process. Each unit of analysis is a multi-turn conversation between an adversarial user and an LLM. The outcome is the number of adversarial turns until the first incorrect answer (count regression) or the round index at first failure (survival analysis), with right-censoring for conversations that never fail. Predictors include model, initial prompt, semantic drift (cosine distance between prompt embeddings), and prompt complexity. We fit Poisson and Negative Binomial regression models, Cox Proportional Hazards models, and frailty models with random effects for conversation. Data is split into train/validation/test sets stratified by initial prompt.

\section{Experiment Design}
\textbf{Units, Outcomes, and Censoring:} Each data point is a multi-turn conversation. Outcomes are adversarial turns until failure or time-to-event, with right-censoring at max round.\newline
\textbf{Predictors:} Model, Prompt$_0$, Model$\times$Prompt$_0$ interaction, semantic drift, prompt complexity.\newline
\textbf{Data Structuring:} Static (conversation-level) and long (turn-level) tables.\newline
\textbf{Splits:} Stratified train/validation/test or k-fold cross-validation.\newline
\textbf{Modeling:} Baseline (Poisson, NegBin, CoxPH) and advanced (frailty/GLMM) models.\newline
\textbf{Evaluation:} AIC/BIC, deviance, RMSE, C-index, hypothesis tests.\newline
\textbf{Visualization:} Kaplan–Meier curves, ICR plots, partial effect plots, interaction heatmaps.

\section{Results}
% Insert tables and figures generated by your pipeline here.
% Example:
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.45\textwidth]{figs/km_by_model.png}
% \caption{Kaplan–Meier survival curves by model.}
% \end{figure}

% \begin{table}[h]
% \centering
% \begin{tabular}{lccc}
% \toprule
% Model & AIC & C-index & Overdispersion \\
% \midrule
% Poisson & ... & ... & ... \\
% NegBin & ... & ... & ... \\
% CoxPH & ... & ... & ... \\
% Frailty & ... & ... & ... \\
% \bottomrule
% \end{tabular}
% \caption{Model comparison metrics.}
% \end{table}

\section{Discussion}
Our results demonstrate that frailty models provide more reliable estimates of LLM robustness by accounting for latent conversation-level vulnerability. Semantic drift is a significant predictor of failure risk, and model $\times$ prompt interactions reveal substantial heterogeneity in robustness. We discuss implications for LLM safety evaluation and future research.

\section{Conclusion}
We present a comprehensive, reproducible framework for evaluating LLM robustness to adversarial prompting using survival and count models. Our approach enables nuanced benchmarking and can inform the design of safer, more robust LLMs.

\section*{Acknowledgments}
% Add funding, collaborators, or data source acknowledgments here.

\bibliography{aaai24}
\bibliographystyle{aaai24}

\end{document} 