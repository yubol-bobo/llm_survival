#!/usr/bin/env python3
"""
Time-Varying Advanced Modeling with Cumulative Drift √ó Base Prompt Interactions
===============================================================================
Implements Cox time-varying models with interaction terms between:
- Base prompt types (C(base_id)) √ó Cumulative drift
- Plus main effects for adversarial prompt type, prompt-to-prompt drift, and context-to-prompt drift

Formula: C(base_id) * cumulative_drift + C(adv_id) + prompt_to_prompt_drift + context_to_prompt_drift

Usage:
    python time_varying_advanced_modeling_cumulative.py

Outputs:
    - Individual model interaction coefficients
    - Interaction effect visualizations
    - Comparison with baseline time-varying models
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from lifelines import CoxTimeVaryingFitter
from tqdm import tqdm
import warnings
import json
import lifelines.utils
warnings.filterwarnings('ignore')

class TimeVaryingAdvancedCumulativeAnalyzer:
    """Advanced time-varying Cox models with cumulative drift √ó base prompt interactions."""
    
    def __init__(self):
        self.models_data = {}
        self.interaction_results = {}
        self.baseline_results = {}
        
    def load_and_prepare_data(self):
        print("\nüîç LOADING AND PREPARING ADVANCED TIME-VARYING DATA (CUMULATIVE)")
        print("=" * 65)
        processed_dir = 'processed_data'
        if not os.path.exists(processed_dir):
            print(f"‚ùå Processed data directory not found: {processed_dir}")
            return False
        model_dirs = [d for d in os.listdir(processed_dir) if os.path.isdir(os.path.join(processed_dir, d))]
        for model_name in tqdm(model_dirs, desc="Loading and preparing models"):
            long_file = os.path.join(processed_dir, model_name, f'{model_name}_long.csv')
            if os.path.exists(long_file):
                try:
                    long_df = pd.read_csv(long_file)
                    required_cols = ['conversation_id', 'round', 'failure', 'prompt_to_prompt_drift', 'context_to_prompt_drift', 'cumulative_drift']
                    if not all(col in long_df.columns for col in required_cols):
                        print(f"‚ö†Ô∏è {model_name}: Missing required drift columns. Skipping.")
                        continue
                    long_df['turn_start'] = long_df.groupby('conversation_id').cumcount()
                    long_df['turn_stop'] = long_df['turn_start'] + 1
                    long_df['fail'] = long_df['failure']
                    long_df.rename(columns={'conversation_id': 'convo_id'}, inplace=True)
                    # Assign adv_id and base_id as before
                    def assign_adv_id(row):
                        if row['turn_start'] == 0:
                            return 'base'
                        else:
                            adv_prompts = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']
                            idx = (int(row['convo_id']) + int(row['turn_start'])) % len(adv_prompts)
                            return adv_prompts[idx]
                    long_df['adv_id'] = long_df.apply(assign_adv_id, axis=1)
                    base_categories = ['STEM', 'Humanities', 'Medical', 'Business', 'Legal']
                    long_df['base_id'] = long_df['convo_id'].apply(
                        lambda x: base_categories[hash(str(x)) % len(base_categories)]
                    )
                    long_df['prompt_to_prompt_drift'] = long_df['prompt_to_prompt_drift'].fillna(0)
                    long_df['context_to_prompt_drift'] = long_df['context_to_prompt_drift'].fillna(0)
                    long_df['cumulative_drift'] = long_df['cumulative_drift'].fillna(0)
                    long_df = long_df.dropna(subset=['convo_id', 'turn_start', 'turn_stop', 'fail'])
                    self.models_data[model_name] = long_df
                    print(f"‚úÖ {model_name}: {len(long_df.convo_id.unique())} conversations, {len(long_df)} turns prepared")
                except Exception as e:
                    print(f"‚ùå Error processing {model_name}: {e}")
            else:
                print(f"‚ö†Ô∏è {model_name}: Long file not found. Skipping.")
        print(f"\nüìä Successfully prepared data for {len(self.models_data)} models.")
        return len(self.models_data) > 0

    def fit_baseline_time_varying_models(self):
        print("\nüèóÔ∏è FITTING BASELINE TIME-VARYING MODELS (NO INTERACTIONS)")
        print("=" * 65)
        for model_name, df in self.models_data.items():
            print(f"--- Baseline model for: {model_name.upper()} ---")
            try:
                ctv = CoxTimeVaryingFitter(penalizer=0.01)
                baseline_formula = "C(adv_id) + C(base_id) + prompt_to_prompt_drift + context_to_prompt_drift + cumulative_drift"
                print(f"    Formula: {baseline_formula}")
                ctv.fit(
                    df, 
                    id_col="convo_id",
                    event_col="fail",
                    start_col="turn_start",
                    stop_col="turn_stop",
                    formula=baseline_formula
                )
                risk_scores = ctv.predict_partial_hazard(df)
                c_index = lifelines.utils.concordance_index(
                    df['turn_stop'],
                    -risk_scores,
                    df['fail']
                )
                self.baseline_results[model_name] = {
                    'summary': ctv.summary,
                    'formula': baseline_formula,
                    'aic': getattr(ctv, 'AIC_partial_', np.nan),
                    'log_likelihood': ctv.log_likelihood_,
                    'n_observations': len(df),
                    'n_events': df['fail'].sum(),
                    'n_conversations': df['convo_id'].nunique(),
                    'cindex': c_index
                }
                print(f"    ‚úÖ Baseline converged. Log-likelihood: {ctv.log_likelihood_:.2f}")
            except Exception as e:
                print(f"    ‚ùå Baseline failed: {e}")
                self.baseline_results[model_name] = None

    def fit_interaction_time_varying_models(self):
        print("\nüî¨ FITTING ADVANCED TIME-VARYING MODELS (WITH CUMULATIVE DRIFT √ó BASE PROMPT INTERACTIONS)")
        print("=" * 70)
        for model_name, df in self.models_data.items():
            print(f"--- Interaction model for: {model_name.upper()} ---")
            try:
                ctv = CoxTimeVaryingFitter(penalizer=0.01)
                interaction_formula = "C(adv_id) * cumulative_drift + C(base_id) + prompt_to_prompt_drift + context_to_prompt_drift"
                print(f"    Formula: {interaction_formula}")
                ctv.fit(
                    df, 
                    id_col="convo_id",
                    event_col="fail",
                    start_col="turn_start",
                    stop_col="turn_stop",
                    formula=interaction_formula
                )
                risk_scores = ctv.predict_partial_hazard(df)
                c_index = lifelines.utils.concordance_index(
                    df['turn_stop'],
                    -risk_scores,
                    df['fail']
                )
                self.interaction_results[model_name] = {
                    'summary': ctv.summary,
                    'formula': interaction_formula,
                    'aic': getattr(ctv, 'AIC_partial_', np.nan),
                    'log_likelihood': ctv.log_likelihood_,
                    'n_observations': len(df),
                    'n_events': df['fail'].sum(),
                    'n_conversations': df['convo_id'].nunique(),
                    'model_object': ctv,
                    'cindex': c_index
                }
                print(f"    ‚úÖ Interaction model converged. Log-likelihood: {ctv.log_likelihood_:.2f}")
                summary_df = ctv.summary
                interaction_terms = summary_df[summary_df.index.str.contains(':', na=False)]
                if not interaction_terms.empty:
                    print(f"    üìä Key interaction terms found: {len(interaction_terms)}")
                    for idx, row in interaction_terms.head(3).iterrows():
                        hr = row['exp(coef)']
                        p_val = row['p']
                        significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns"
                        print(f"      ‚Ä¢ {idx}: HR={hr:.3f}, p={p_val:.3f} {significance}")
            except Exception as e:
                print(f"    ‚ùå Interaction model failed: {e}")
                self.interaction_results[model_name] = None

    def compare_models(self):
        """Compare baseline vs interaction models for each LLM."""
        print("\nüìä COMPARING BASELINE VS INTERACTION MODELS (CUMULATIVE)")
        print("=" * 50)
        comparison_results = []
        for model_name in self.models_data.keys():
            baseline = self.baseline_results.get(model_name)
            interaction = self.interaction_results.get(model_name)
            if baseline is None or interaction is None:
                print(f"‚ö†Ô∏è {model_name}: Cannot compare - missing results")
                continue
            comparison = {
                'Model': model_name,
                'Baseline_LogLik': baseline['log_likelihood'],
                'Interaction_LogLik': interaction['log_likelihood'],
                'LogLik_Improvement': interaction['log_likelihood'] - baseline['log_likelihood'],
                'Baseline_AIC': baseline.get('aic', np.nan),
                'Interaction_AIC': interaction.get('aic', np.nan),
                'AIC_Improvement': baseline.get('aic', np.nan) - interaction.get('aic', np.nan),
                'N_Observations': baseline['n_observations'],
                'N_Events': baseline['n_events'],
                'N_Conversations': baseline['n_conversations'],
                'Interaction_Formula': interaction['formula'],
                'Note': interaction.get('note', 'Full interaction model')
            }
            comparison_results.append(comparison)
            ll_improve = comparison['LogLik_Improvement']
            aic_improve = comparison['AIC_Improvement']
            print(f"‚úÖ {model_name}:")
            print(f"   ‚Ä¢ Log-likelihood improvement: {ll_improve:+.2f}")
            print(f"   ‚Ä¢ AIC improvement: {aic_improve:+.2f}")
            print(f"   ‚Ä¢ Formula: {interaction['formula'][:60]}...")
        return comparison_results

    def extract_interaction_effects(self):
        """Extract and summarize interaction effects across models (cumulative)."""
        print("\nüîç EXTRACTING INTERACTION EFFECTS ACROSS MODELS (CUMULATIVE)")
        print("=" * 55)
        interaction_effects = []
        for model_name, results in self.interaction_results.items():
            if results is None:
                continue
            summary_df = results['summary']
            main_effects = summary_df[~summary_df.index.str.contains(':', na=False)]
            interaction_terms = summary_df[summary_df.index.str.contains(':', na=False)]
            print(f"\nü§ñ {model_name.upper()} (CUMULATIVE):")
            print(f"   ‚Ä¢ Main effects: {len(main_effects)}")
            print(f"   ‚Ä¢ Interaction terms: {len(interaction_terms)}")
            for term, row in interaction_terms.iterrows():
                effect_data = {
                    'Model': model_name,
                    'Term': term,
                    'Coefficient': row['coef'],
                    'Hazard_Ratio': row['exp(coef)'],
                    'P_Value': row['p'],
                    'CI_Lower': row['coef lower 95%'],
                    'CI_Upper': row['coef upper 95%'],
                    'HR_CI_Lower': row['exp(coef) lower 95%'],
                    'HR_CI_Upper': row['exp(coef) upper 95%'],
                    'Significance': '***' if row['p'] < 0.001 else '**' if row['p'] < 0.01 else '*' if row['p'] < 0.05 else 'ns'
                }
                interaction_effects.append(effect_data)
                if row['p'] < 0.05:
                    hr = row['exp(coef)']
                    p_val = row['p']
                    sig = effect_data['Significance']
                    print(f"      üéØ {term}: HR={hr:.3f}, p={p_val:.4f} {sig}")
        return interaction_effects

    # (The rest of the script remains unchanged: compare_models, extract_interaction_effects, save_results, create_visualizations, etc.)
    # ...
    # For brevity, you can copy the rest of the methods from the original script.

    def run_complete_analysis(self):
        print("üî¨ TIME-VARYING ADVANCED MODELING WITH CUMULATIVE DRIFT √ó BASE PROMPT INTERACTIONS")
        print("=" * 60)
        print("Implementing interaction terms between base prompt types and cumulative drift")
        print("Formula: C(base_id) * cumulative_drift + C(adv_id) + prompt_to_prompt_drift + context_to_prompt_drift\n")
        if not self.load_and_prepare_data():
            print("‚ùå Data preparation failed")
            return
        self.fit_baseline_time_varying_models()
        self.fit_interaction_time_varying_models()
        self.save_results()
        # (Call the rest of the methods as in the original script)

    def save_results(self):
        """Save all results to files with 'cumulative' in the filenames."""
        print("\nüíæ SAVING TIME-VARYING ADVANCED MODELING RESULTS (CUMULATIVE)")
        print("=" * 55)
        os.makedirs('generated/outputs', exist_ok=True)
        # Save baseline results
        if self.baseline_results:
            baseline_data = []
            for model_name, results in self.baseline_results.items():
                if results is not None:
                    summary_df = results['summary'].copy()
                    summary_df['Model'] = model_name
                    summary_df['Analysis_Type'] = 'Baseline_TimeVarying_Cumulative'
                    baseline_data.append(summary_df)
            if baseline_data:
                pd.concat(baseline_data).to_csv('generated/outputs/baseline_time_varying_results_cumulative.csv')
                print("‚úÖ Baseline time-varying results saved (cumulative)")
        # Save interaction results
        if self.interaction_results:
            interaction_data = []
            for model_name, results in self.interaction_results.items():
                if results is not None:
                    summary_df = results['summary'].copy()
                    summary_df['Model'] = model_name
                    summary_df['Analysis_Type'] = 'Interaction_TimeVarying_Cumulative'
                    summary_df['Formula'] = results['formula']
                    interaction_data.append(summary_df)
            if interaction_data:
                pd.concat(interaction_data).to_csv('generated/outputs/interaction_time_varying_results_cumulative.csv')
                print("‚úÖ Interaction time-varying results saved (cumulative)")
        # Save comparison results
        comparison_results = self.compare_models()
        if comparison_results:
            pd.DataFrame(comparison_results).to_csv('generated/outputs/model_comparison_time_varying_cumulative.csv', index=False)
            print("‚úÖ Model comparison results saved (cumulative)")
        # Save interaction effects summary
        interaction_effects = self.extract_interaction_effects()
        if interaction_effects:
            pd.DataFrame(interaction_effects).to_csv('generated/outputs/interaction_effects_summary_cumulative.csv', index=False)
            print("‚úÖ Interaction effects summary saved (cumulative)")
        # Save C-index for all models
        cindex_rows = []
        for model_name in self.baseline_results:
            base = self.baseline_results.get(model_name)
            inter = self.interaction_results.get(model_name)
            cindex_rows.append({
                'Model': model_name,
                'C_index_Baseline': base['cindex'] if base else np.nan,
                'C_index_Interaction': inter['cindex'] if inter else np.nan
            })
        if cindex_rows:
            cindex_df = pd.DataFrame(cindex_rows)
            cindex_file = 'generated/outputs/time_varying_advanced_cindex_cumulative.csv'
            cindex_df.to_csv(cindex_file, index=False)
            print(f"‚úÖ C-index values saved to: {cindex_file}")


def main():
    analyzer = TimeVaryingAdvancedCumulativeAnalyzer()
    analyzer.run_complete_analysis()

if __name__ == "__main__":
    main() 